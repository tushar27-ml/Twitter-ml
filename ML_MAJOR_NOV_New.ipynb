#Load the drive
from google.colab import drive
drive.mount('/content/drive')
#Import all the necessary packages
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
!pip install pyspellchecker
from spellchecker import SpellChecker
file_name = r"/content/drive/MyDrive/Information.csv"
#Having a glimpse of the dataset
dataset = pd.read_csv(file_name,encoding = "ISO-8859-1", engine = 'python')
dataset.head()
#Viewing all the null values in the dataset
dataset.isna().sum()
dataset.shape
#Removing the columns with all null values
data = dataset.drop(["gender_gold", "profile_yn_gold", "tweet_coord", "user_timezone", "tweet_location"], axis = 1)
data = data[data["gender"] != "unknown"]
#Dropping remaining null values
data = data.dropna()
data = data.reset_index()
data.shape
data.head(3)
graph = sns.catplot(x = "gender", y = "tweet_count", data = data, kind = "point")
fontlabs = {'size' :16}
graph = plt.xticks(fontsize = 15)
graph = plt.yticks(fontsize = 15)
graph = plt.xlabel("Gender", fontdict = fontlabs)
graph = plt.ylabel("No. of Tweets", fontdict = fontlabs)
graph = plt.title("Total Tweets", fontdict = {'size' : 20})
# Brands tweet more than any individual
checker = SpellChecker()
x = []
tweet1 = data['text'][15]
tweetlist = tweet1.lower().split()
s = 0
for word in tweetlist:
    if word != checker.correction(word):
      s = 1
      break
if s == 1:
  x.append(1)  # There is a typo
else:
  x.append(0)  # There is no typo  

x

tweet1
checker = SpellChecker()
X = []
for i in range(len(data['text'])):
  if i % 100 == 0:
    print(i)
  tweet1 = data['text'][i]
  tweetlist = tweet1.lower().split()
  s = 0
  for word in tweetlist:
    if word != checker.correction(word):
      s = 1
      break
  if s == 1:
    X.append(1)  # There is a typo
  else:
    X.append(0)  # There is no typo  

sum(X)
data2 = data[data["gender"] != "brand"]
data2 = data2.drop(['level_0'], axis = 1)
data2 = data2.reset_index()
data2.shape
data2.head()
data2.shape
checker = SpellChecker()
x = []
for i in range(len(data2['text'])):
  if i % 100 == 0:
    print(i)
  tweet1 = data['text'][i]
  tweetlist = tweet1.lower().split()
  s = 0
  for word in tweetlist:
    if word != checker.correction(word):
      s = 1
      break
  if s == 1:
    x.append(1)  # There is a typo
  else:
    x.append(0)  # There is no typo
# Import libraries for making the model
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import VotingClassifier

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
data["tweet_typo"] = X
data2["tweet_typo"] = x
data.to_csv(r"/content/drive/MyDrive/Information-data.csv")
data2.to_csv(r"/content/drive/MyDrive/Information-data2.csv")
### Information-data.csv contains 3 genders including brand
### Information-data2.csv contains 2 genders only
data_1 = pd.read_csv(r"/content/drive/MyDrive/Information-data.csv")
data_2 = pd.read_csv(r"/content/drive/MyDrive/Information-data2.csv")
data_1.head()
data_final_1 = data_1[["_unit_id", "_trusted_judgments", "gender", "gender:confidence", "profile_yn:confidence", "fav_number", "retweet_count", "tweet_count", "tweet_typo"]]
data_final_2 = data_2[["_unit_id", "_trusted_judgments", "gender", "gender:confidence", "profile_yn:confidence", "fav_number", "retweet_count", "tweet_count", "tweet_typo"]]
le = LabelEncoder()
#data_final_1["gender"] = le.fit_transform(data_final_1["gender"])
data_final_2["gender"] = le.fit_transform(data_final_2["gender"])
data_final_2.head()

# 1 - Male
# 0 - Female
y = data_final_2['gender'].values
X = data_final_2[[column for column in data_final_2.columns if column not in ['gender']]]
X.head()
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 19)
logclf = LogisticRegression(random_state = 19)
knn = KNeighborsClassifier(n_neighbors=27)
dt = DecisionTreeClassifier(
    #bootstrap = False,
    max_depth = 10,
    max_features = 'sqrt',
    min_samples_leaf = 2,
    min_samples_split = 10,
    #n_estimators = 200,
    random_state = 19
)
logclf.fit(X_train, y_train)
y_pred = logreg.predict(X_test)
accuracy_score(y_test, y_pred)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
accuracy_score(y_test, y_pred)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
accuracy_score(y_test, y_pred)
rf = RandomForestClassifier(random_state = 19)

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, random_state = 19, n_jobs = -1)
# Fit the random search model
rf_random.fit(X,y)
rf_random.best_params_
rf = RandomForestClassifier(
    bootstrap = False,
    max_depth = 10,
    max_features = 'sqrt',
    min_samples_leaf = 2,
    min_samples_split = 10,
    n_estimators = 200
)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy_score(y_pred, y_test)
vot_clf = VotingClassifier(estimators = [('lr', logclf), ('knn', knn), ('dt', dt), ('rf', rf)], voting = 'hard')
vot_clf.fit(X_train, y_train)
y_pred = vot_clf.predict(X_test)
accuracy_score(y_pred, y_test)
cm = confusion_matrix(y_test, y_pred)
def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):
    import itertools   

    accuracy = np.trace(cm) / np.sum(cm).astype('float')
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()
plot_confusion_matrix(cm, ['Female', 'Male'])
# In conclusion 
# Females make more typos than men
